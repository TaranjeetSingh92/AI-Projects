{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca8ab347-ca02-4b70-9e6d-5016852cd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "#print(\"Imported all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "847f7db6-6636-4363-b418-8731eef2ccd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists: C:\\Users\\User\\ChatBot.txt.txt\n",
      "C:\\Users\\User\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = r\"C:\\Users\\User\\ChatBot.txt.txt\"\n",
    "if os.path.exists(path):\n",
    "    print(f\"The file exists: {path}\")\n",
    "else:\n",
    "    print(f\"The file does not exist: {path}\")\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5215c01f-f8aa-434f-a9fc-fd360fa49691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for single file\n",
    "'''\n",
    "f=open(\"C:/Users/User/ChatBot.txt.txt\",'r',errors='ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower() \n",
    "nltk.download('punkt') #pre-trained tokenizer and sentence boundary detector\n",
    "nltk.download('wordnet')  #lexical database for the English\n",
    "sent_tokens=nltk.sent_tokenize(raw) #Raw data to sentences\n",
    "word_tokens=nltk.word_tokenize(raw) #Raw data to words\n",
    "#print(\"Done\")\n",
    "'''\n",
    "# Ensure required NLTK resources are downloaded\n",
    "nltk.download('punkt')  # Pre-trained tokenizer and sentence boundary detector\n",
    "nltk.download('wordnet')  # Lexical database for the English language\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    all_raw_text = \"\"\n",
    "    \n",
    "    # Iterate over each file in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)  # Construct full file path\n",
    "        if os.path.isfile(file_path):  # Check if it's a file\n",
    "            try:\n",
    "                with open(file_path, 'r', errors='ignore') as f:\n",
    "                    raw = f.read()\n",
    "                    all_raw_text += raw + \" \"  # Append the content of the file to the combined text\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Not a file: {file_path}\")\n",
    "    \n",
    "    # Convert the combined raw text to lowercase\n",
    "    all_raw_text = all_raw_text.lower()\n",
    "    \n",
    "    # Tokenize the combined text into sentences and words\n",
    "    sent_tokens = nltk.sent_tokenize(all_raw_text)  # Raw data to sentences\n",
    "    word_tokens = nltk.word_tokenize(all_raw_text)  # Raw data to words\n",
    "    \n",
    "    return sent_tokens, word_tokens\n",
    "\n",
    "# Specify the directory containing the files to be processed\n",
    "directory_path = \"C:/Users/User/ChatBotFiles\"\n",
    "\n",
    "# Process the directory and obtain tokens\n",
    "sent_tokens, word_tokens = process_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41647ec3-bc02-4dd2-bb81-2f2283b49311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a chatbot (originally chatterbot)[1] is a software application or web interface that is designed to mimic human conversation through text or voice interactions.', '[2][3][4] modern chatbots are typically online and use generative artificial intelligence systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner.', 'such chatbots often use deep learning and natural language processing, but simpler chatbots have existed for decades.', \"since late 2022, the field has gained widespread attention due to the popularity of openai's chatgpt,[5][6] followed by alternatives such as microsoft's copilot and google's gemini.\", '[7] such examples reflect the recent practice of basing such products upon broad foundational large language models, such as gpt-4 or the gemini language model, that get fine-tuned so as to target specific tasks or applications (i.e., simulating human conversation, in the case of chatbots).']\n",
      "['a', 'chatbot', '(', 'originally', 'chatterbot']\n"
     ]
    }
   ],
   "source": [
    "#checking if above is working fine\n",
    "print(sent_tokens[:5])\n",
    "print(word_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75483289-789e-4bf2-96d4-d0b2da69eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLemmatization is the process of reducing a word to its base or dictionary form, called a lemma.\\neg: word- flies then Lemma-fly\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing and Handling Raw Text\n",
    "\n",
    "# Initialize the WordNetLemmatizer from nltk for lemmatization\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Define a function to lemmatize tokens (words)\n",
    "def LemTokens(tokens):\n",
    "    # Apply lemmatization to each token (word) in the list and return the list of lemmatized tokens\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Create a dictionary to map each punctuation character to None for removal\n",
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# Define a function to normalize text by tokenizing, lowercasing, and removing punctuation\n",
    "def LemNormalize(text):\n",
    "    # Convert text to lowercase, remove punctuation, and tokenize\n",
    "    # Then apply lemmatization to the tokens and return the normalized tokens\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n",
    "\n",
    "'''\n",
    "Lemmatization is the process of reducing a word to its base or dictionary form, called a lemma.\n",
    "eg: word- flies then Lemma-fly\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a11a464e-2f6e-4cda-8a12-0209671e11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greetings\n",
    "Greeting_inputs=(\"hello\",\"hi\",\"greetings\",\"sup\",\"morning\",\"hey\")\n",
    "Greeting_responses=[\"hi\",\"hey\",\"nods\",\"hi there\",\"hello\",\"Hi, morning\",\"Welcome\",\"Glad, you sent a message!\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in Greeting_inputs:\n",
    "            return random.choice(Greeting_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a9eeeb2-8887-42e5-aec4-a3676f2bca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use TF-IDF- Term Frequency-Inverse Document Frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#find similarty b/w user and raw document\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9ea2b3c-7ef9-4e0a-947a-2fafe815b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a response based on user input\n",
    "def responses(user_response):\n",
    "    robo_response = ''  # Initialize an empty string for the response\n",
    "    \n",
    "    # Append the user response to the list of sentence tokens\n",
    "    sent_tokens.append(user_response)\n",
    "    \n",
    "    # Initialize TfidfVectorizer with the LemNormalize tokenizer and English stop words\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")\n",
    "    \n",
    "    # Fit the TfidfVectorizer on the sentence tokens and transform them into TF-IDF matrices\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    \n",
    "    # Compute cosine similarity between the last sentence (user response) and all sentences\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    \n",
    "    # Get the index of the sentence with the highest similarity (excluding the last added user response)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    \n",
    "    # Flatten the similarity values array and sort it\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    \n",
    "    # Retrieve the second highest similarity score\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    # Check if the highest similarity score is zero\n",
    "    if req_tfidf == 0:\n",
    "        # If similarity is zero, append a default response indicating lack of understanding\n",
    "        robo_response = robo_response + \"I am sorry! I don't understand you.\"\n",
    "    else:\n",
    "        # Otherwise, append the most similar sentence from the sent_tokens to the response\n",
    "        robo_response = robo_response + sent_tokens[idx]\n",
    "    \n",
    "    # Return the generated response\n",
    "    return robo_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8191d1e-0a14-46b5-a64c-e0fba8604272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:My Name is ChatBot1. I can help you with India, Space Flights and ChatBot, If you want to quit type Bye !!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is india\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:[117] the sultanate was to control much of north india and to make many forays into south india.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is space \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:objectives for space missions may include space exploration, space research, and national firsts in spaceflight.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is chatbot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:an example of a gpt chatbot is chatgpt.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how many states in india\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:in 1956, under the states reorganisation act, states were reorganised on a linguistic basis.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " union territoy in india\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:[266]\n",
      "administrative divisions\n",
      "main article: administrative divisions of india\n",
      "see also: political integration of india\n",
      "india is a federal union comprising 28 states and 8 union territories.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " who is prime minister\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:there were two prime ministers during this period; v.p.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what are biodiversity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:[69] india's land is megadiverse, with four biodiversity hotspots.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:Thanks for converation, Bye!\n"
     ]
    }
   ],
   "source": [
    "#Conversation b/w user and bot\n",
    "flag= True\n",
    "print(\"ChatBot:My Name is ChatBot1. I can help you with India, Space Flights and ChatBot, If you want to quit type Bye !!\")\n",
    "while(flag==True):\n",
    "    user_response=input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response==\"thanks\" or user_response ==\"thank you\"):\n",
    "            flag=False\n",
    "            print(\"ChatBot:You are Welcome!\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ChatBot:\"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ChatBot:\",end=\"\")\n",
    "                print(responses(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ChatBot:Thanks for converation, Bye!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b2a3f-e4d0-45d1-bd40-1b9f82472eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
